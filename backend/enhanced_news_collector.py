# backend/enhanced_news_collector.py

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from datetime import datetime

import feedparser
import requests
from bs4 import BeautifulSoup
from kiwipiepy import Kiwi # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°

from database import db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (ëª…ì‚¬ë§Œ ì¶”ì¶œ)
kiwi = Kiwi()
kiwi.prepare()

# ìˆ˜ì§‘í•  RSS í”¼ë“œ ëª©ë¡
FEEDS = [
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìì‹ ë¬¸_ì†ë³´"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter"},
    {"feed_url": "https://byline.network/feed/", "source": "Byline Network"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum"},
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge"},
    {"feed_url": "https://www.wired.com/feed/rss", "source": "WIRED"},
]

class EnhancedNewsCollector:
    def __init__(self):
        self.session = requests.Session()
        self.stats = {}

    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """Kiwië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not text:
            return []
        
        # í˜•íƒœì†Œ ë¶„ì„ ì‹¤í–‰ (NNG: ì¼ë°˜ ëª…ì‚¬, NNP: ê³ ìœ  ëª…ì‚¬)
        result = kiwi.analyze(text[:1000], top_n=top_n, pos_score=0.8)
        
        keywords = []
        if result and result[0]:
            for token, pos, _, _ in result[0][0]:
                if pos in ('NNG', 'NNP') and len(token) > 1:
                    keywords.append(token)
        
        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
        return sorted(list(set(keywords)), key=keywords.index)


    def _process_entry(self, entry: Dict, source: str) -> Optional[Dict]:
        """ê°œë³„ ë‰´ìŠ¤ í•­ëª©ì„ ì²˜ë¦¬í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            title = entry.get("title", "No Title").strip()
            link = entry.get("link", "").strip()
            if not title or not link:
                return None

            published_str = entry.get("published", datetime.now().isoformat())
            try:
                import dateutil.parser
                published = dateutil.parser.parse(published_str).isoformat()
            except:
                published = datetime.now().isoformat()

            # HTML íƒœê·¸ ì œê±° ë° ìš”ì•½ ì •ë¦¬
            summary_html = entry.get("summary", "")
            summary = BeautifulSoup(summary_html, "html.parser").get_text(separator=' ', strip=True)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(f"{title} {summary}")

            return {
                'title': title, 'link': link, 'published': published,
                'source': source, 'summary': summary, 'keywords': keywords
            }
        except Exception as e:
            logger.error(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({entry.get('link')}): {e}")
            return None

    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        """ë‹¨ì¼ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        feed_url = feed_config.get("feed_url")
        source = feed_config.get("source", "Unknown")
        logger.info(f"ğŸ“¡ {source}ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
            response = self.session.get(feed_url, timeout=20)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            
            if not feed or not feed.entries:
                logger.warning(f"âŒ {source}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            articles = [self._process_entry(entry, source) for entry in feed.entries[:20]]
            valid_articles = [article for article in articles if article]
            
            logger.info(f"âœ… {source}: {len(valid_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ.")
            return valid_articles
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜): {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨ (ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜): {e}")
            return []

    def save_articles(self, articles: List[Dict]) -> Dict:
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        stats = {'inserted': 0, 'updated': 0, 'skipped': 0}
        for article in articles:
            try:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ë¥¼ ì‚½ì…í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                result = db.insert_or_update_article(article)
                stats[result] += 1
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì˜¤ë¥˜ ({article.get('link')}): {e}")
                stats['skipped'] += 1
        return stats

    def collect_all_news(self, max_feeds: Optional[int] = None) -> Dict:
        """ëª¨ë“  RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        self.stats = {'total_processed': 0, 'total_inserted': 0, 'total_updated': 0, 'total_skipped': 0}
        start_time = time.time()
        
        feeds_to_process = FEEDS[:max_feeds] if max_feeds else FEEDS
        
        all_articles = []
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_feed = {executor.submit(self.collect_from_feed, feed): feed for feed in feeds_to_process}
            for future in as_completed(future_to_feed):
                all_articles.extend(future.result())
        
        unique_articles = list({article['link']: article for article in all_articles}.values())
        logger.info(f"ğŸ“Š ì´ {len(unique_articles)}ê°œì˜ ê³ ìœ í•œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        if unique_articles:
            save_stats = self.save_articles(unique_articles)
            self.stats.update(save_stats)
        
        duration = time.time() - start_time
        self.stats['total_processed'] = len(unique_articles)
        logger.info(f"âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")
        return {'status': 'success', 'duration': duration, 'stats': self.stats}

# ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
collector = EnhancedNewsCollector()
