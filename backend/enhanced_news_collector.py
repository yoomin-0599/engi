# backend/enhanced_news_collector.py (ì •êµí•œ í‚¤ì›Œë“œ ì‚¬ì „ì´ ë°˜ì˜ëœ ìµœì¢… ë²„ì „)

import logging
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from datetime import datetime

import feedparser
import requests
from bs4 import BeautifulSoup
import dateutil.parser

from database import db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Enhanced configuration
MAX_RESULTS = int(os.getenv("MAX_RESULTS", "15"))
MAX_TOTAL_PER_SOURCE = int(os.getenv("MAX_TOTAL_PER_SOURCE", "200"))
RSS_BACKFILL_PAGES = int(os.getenv("RSS_BACKFILL_PAGES", "3"))

CONNECT_TIMEOUT = float(os.getenv("CONNECT_TIMEOUT", "10.0"))
READ_TIMEOUT = float(os.getenv("READ_TIMEOUT", "15.0"))

ENABLE_SUMMARY = os.getenv("ENABLE_SUMMARY", "false").lower() == "true"
ENABLE_HTTP_CACHE = os.getenv("ENABLE_HTTP_CACHE", "true").lower() == "true"
HTTP_CACHE_EXPIRE = int(os.getenv("HTTP_CACHE_EXPIRE", "3600"))
PARALLEL_MAX_WORKERS = int(os.getenv("PARALLEL_MAX_WORKERS", "8"))
SKIP_UPDATE_IF_EXISTS = os.getenv("SKIP_UPDATE_IF_EXISTS", "true").lower() == "true"

STRICT_TECH_KEYWORDS = os.getenv("STRICT_TECH_KEYWORDS", "true").lower() == "true"
SKIP_NON_TECH = os.getenv("SKIP_NON_TECH", "false").lower() == "true"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# --- 1. ì‚¬ìš©ìž ì œê³µ ì¹´í…Œê³ ë¦¬ ë° í‚¤ì›Œë“œ ì‚¬ì „ ---
CATEGORIES = {
    "ì²¨ë‹¨ ì œì¡°Â·ê¸°ìˆ  ì‚°ì—…": {
        "ë°˜ë„ì²´": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "dram", "nand", "hbm", "íŒŒìš´ë“œë¦¬", "foundry", "euv"],
        "ìžë™ì°¨": ["ìžë™ì°¨", "ì „ê¸°ì°¨", "ev", "ìˆ˜ì†Œì°¨", "í•˜ì´ë¸Œë¦¬ë“œ", "ìžìœ¨ì£¼í–‰", "adas", "ëª¨ë¹Œë¦¬í‹°"],
        "ì´ì°¨ì „ì§€": ["ì´ì°¨ì „ì§€", "2ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ess", "ì „ê³ ì²´", "ncm", "lfp", "ì–‘ê·¹ìž¬", "ìŒê·¹ìž¬"],
        "ë””ìŠ¤í”Œë ˆì´": ["ë””ìŠ¤í”Œë ˆì´", "oled", "amoled", "lcd", "qd", "ë§ˆì´í¬ë¡œ led"],
        "ë¡œë´‡Â·ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬": ["ë¡œë´‡", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ì‚°ì—…ìš© ë¡œë´‡", "í˜‘ë™ë¡œë´‡", "cobot", "ë””ì§€í„¸íŠ¸ìœˆ"],
    },
    "ë””ì§€í„¸Â·ICT ì‚°ì—…": {
        "AI": ["ai", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìƒì„±í˜• ai", "ì±—ë´‡", "llm"],
        "ICTÂ·í†µì‹ ": ["5g", "6g", "ë„¤íŠ¸ì›Œí¬", "í†µì‹ ", "ìœ„ì„±í†µì‹ ", "í´ë¼ìš°ë“œ", "ë°ì´í„°ì„¼í„°", "ì—£ì§€ ì»´í“¨íŒ…"],
        "ì†Œí”„íŠ¸ì›¨ì–´Â·í”Œëž«í¼": ["ì†Œí”„íŠ¸ì›¨ì–´", "ë©”íƒ€ë²„ìŠ¤", "vr", "ar", "xr", "saas", "í•€í…Œí¬", "í”Œëž«í¼", "ott", "ê²Œìž„", "ë³´ì•ˆ", "ë¹…ë°ì´í„°", "ë¸”ë¡ì²´ì¸"],
    },
}

# --- 2. [ì¶”ê°€] ì‚¬ìš©ìž ì œê³µ ê¸°ìˆ  í‚¤ì›Œë“œ ë° ë¶ˆìš©ì–´ ì‚¬ì „ ---
STOP_WORDS = {
    "ê¸°ìž", "ë‰´ìŠ¤", "íŠ¹íŒŒì›", "ì˜¤ëŠ˜", "ë§¤ìš°", "ê¸°ì‚¬", "ì‚¬ì§„", "ì˜ìƒ", "ì œê³µ", "ìž…ë ¥",
    "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ì§€ë‚œ", "ì´ë²ˆ", "ê´€ë ¨", "ëŒ€í•œ", "í†µí•´", "ëŒ€í•´", "ìœ„í•´",
    "ìž…ë‹ˆë‹¤", "í•œë‹¤", "í–ˆë‹¤", "í•˜ì˜€ë‹¤", "ì—ì„œëŠ”", "ì—ì„œ", "ëŒ€í•œ", "ì´ë‚ ", "ë¼ë©°", "ë‹¤ê³ ", "ì˜€ë‹¤", "í–ˆë‹¤ê°€", "í•˜ë©°",
    "the", "and", "for", "are", "but", "not", "you", "all", "can", "had", "her", "was", "one", "our"
}

TECH_KEYWORDS = {
    "ai", "ì¸ê³µì§€ëŠ¥", "machine learning", "ë¨¸ì‹ ëŸ¬ë‹", "deep learning", "ë”¥ëŸ¬ë‹",
    "chatgpt", "gpt", "llm", "ìƒì„±í˜•ai", "generative ai", "ì‹ ê²½ë§", "neural network",
    "ë°˜ë„ì²´", "semiconductor", "ë©”ëª¨ë¦¬", "memory", "dram", "nand", "hbm",
    "gpu", "cpu", "npu", "tpu", "fpga", "asic", "ì¹©ì…‹", "chipset",
    "ì‚¼ì„±ì „ìž", "samsung", "skí•˜ì´ë‹‰ìŠ¤", "tsmc", "ì—”ë¹„ë””ì•„", "nvidia",
    "5g", "6g", "lte", "ì™€ì´íŒŒì´", "wifi", "ë¸”ë£¨íˆ¬ìŠ¤", "bluetooth",
    "í´ë¼ìš°ë“œ", "cloud", "ë°ì´í„°ì„¼í„°", "data center", "ì„œë²„", "server",
    "ë„¤íŠ¸ì›Œí¬", "network", "cdn", "api", "sdk",
    "ë¸”ë¡ì²´ì¸", "blockchain", "ì•”í˜¸í™”í", "cryptocurrency", "bitcoin", "ë¹„íŠ¸ì½”ì¸",
    "ethereum", "ì´ë”ë¦¬ì›€", "nft", "defi", "ë©”íƒ€ë²„ìŠ¤", "metaverse",
    "ìžìœ¨ì£¼í–‰", "autonomous", "ì „ê¸°ì°¨", "electric vehicle", "ev", "tesla", "í…ŒìŠ¬ë¼",
    "ë°°í„°ë¦¬", "battery", "ë¦¬íŠ¬", "lithium", "ìˆ˜ì†Œ", "hydrogen",
    "ë³´ì•ˆ", "security", "í•´í‚¹", "hacking", "ì‚¬ì´ë²„", "cyber", "ëžœì„¬ì›¨ì–´", "ransomware",
    "ê°œì¸ì •ë³´", "privacy", "ë°ì´í„°ë³´í˜¸", "gdpr", "ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸", "zero trust",
    "ì˜¤í”ˆì†ŒìŠ¤", "open source", "ê°œë°œìž", "developer", "í”„ë¡œê·¸ëž˜ë°", "programming",
    "python", "javascript", "react", "node.js", "docker", "kubernetes",
}

# Comprehensive RSS feeds (Korean + Global)
FEEDS = [
    # Korean Tech News
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìžì‹ ë¬¸_ì†ë³´", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section901.xml", "source": "ì „ìžì‹ ë¬¸_ì˜¤ëŠ˜ì˜ë‰´ìŠ¤", "category": "IT", "lang": "ko"},
    {"feed_url": "https://zdnet.co.kr/news/news_xml.asp", "source": "ZDNet Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.itworld.co.kr/rss/all.xml", "source": "ITWorld Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.ciokorea.com/rss/all.xml", "source": "CIO Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter", "category": "IT", "lang": "ko"},
    {"feed_url": "https://byline.network/feed/", "source": "Byline Network", "category": "IT", "lang": "ko"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum", "category": "Startup", "lang": "ko"},
    {"feed_url": "https://www.boannews.com/media/news_rss.xml", "source": "ë³´ì•ˆë‰´ìŠ¤", "category": "Security", "lang": "ko"},
    {"feed_url": "https://it.chosun.com/rss.xml", "source": "ITì¡°ì„ ", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.ddaily.co.kr/news_rss.php", "source": "ë””ì§€í„¸ë°ì¼ë¦¬", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.kbench.com/rss.xml", "source": "KBench", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.sedaily.com/rss/IT.xml", "source": "ì„œìš¸ê²½ì œ IT", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.hankyung.com/feed/it", "source": "í•œêµ­ê²½ì œ IT", "category": "IT", "lang": "ko"},
    
    # Global Tech News
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.eetimes.com/feed/", "source": "EE Times", "category": "Electronics", "lang": "en"},
    {"feed_url": "https://spectrum.ieee.org/rss/fulltext", "source": "IEEE Spectrum", "category": "Engineering", "lang": "en"},
    {"feed_url": "https://www.technologyreview.com/feed/", "source": "MIT Tech Review", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss", "source": "WIRED", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.engadget.com/rss.xml", "source": "Engadget", "category": "Tech", "lang": "en"},
    {"feed_url": "https://venturebeat.com/category/ai/feed/", "source": "VentureBeat AI", "category": "AI", "lang": "en"},
    {"feed_url": "https://arstechnica.com/feed/", "source": "Ars Technica", "category": "Tech", "lang": "en"},
    {"feed_url": "https://feeds.feedburner.com/oreilly/radar", "source": "O'Reilly Radar", "category": "Tech", "lang": "en"},
]

class EnhancedNewsCollector:
    def __init__(self):
        self.session = requests.Session()
        self.stats = {}

    def _classify_article(self, text: str) -> Dict:
        """ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        best_match = {'score': 0, 'main': 'ê¸°íƒ€', 'sub': 'ê¸°íƒ€'}
        for main_cat, subcats in CATEGORIES.items():
            for sub_cat, keywords in subcats.items():
                score = sum(1 for kw in keywords if kw in text)
                if score > best_match['score']:
                    best_match = {'score': score, 'main': main_cat, 'sub': sub_cat}
        return {'main_category': best_match['main'], 'sub_category': best_match['sub']}

    def extract_main_text(self, url: str) -> str:
        """ê¸°ì‚¬ URLì— ì§ì ‘ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = self.session.get(url, timeout=20, headers=headers, allow_redirects=True)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            for element in soup(["script", "style", "nav", "footer", "aside", "header"]): element.decompose()
            
            content_selectors = ["article", "[class*='article']", "[id*='content']", "main", "[class*='post-content']"]
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    main_content = element.get_text(separator="\n", strip=True)
                    if len(main_content) > 200: return main_content
            return ""
        except Exception as e:
            logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return ""
    
    def extract_keywords(self, text: str, title: str, top_k: int = 15) -> List[str]:
        """ì •êµí•œ ë°©ì‹ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not text and not title: return []
        combined_text = f"{title} {text}".lower()
        keywords = {kw for kw in TECH_KEYWORDS if kw in combined_text}
        
        patterns = [r'\b[A-Z]{3,}\b', r'[ê°€-íž£]{2,8}(?:ê¸°ìˆ |ì‹œìŠ¤í…œ|í”Œëž«í¼)']
        for pattern in patterns:
            matches = re.findall(pattern, f"{title} {text}")
            keywords.update(matches)
        
        unique_keywords = [kw for kw in list(keywords) if kw.lower() not in STOP_WORDS and len(kw) > 1]
        return unique_keywords[:top_k]

    def process_entry(self, entry: Dict, source: str, language: str) -> Optional[Dict]:
        """ê°œë³„ ë‰´ìŠ¤ í•­ëª©ì„ ì²˜ë¦¬í•˜ê³ , ë¶„ë¥˜ ë° í‚¤ì›Œë“œ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        title = entry.get("title", "No Title").strip()
        link = entry.get("link", "").strip()
        if not title or not link: return None

        try: published = dateutil.parser.parse(entry.get("published", "")).isoformat()
        except: published = datetime.now().isoformat()

        summary = BeautifulSoup(entry.get("summary", ""), "html.parser").get_text(separator=' ', strip=True)
        raw_text = self.extract_main_text(link)
        
        full_text = f"{title} {summary} {raw_text}"
        classification = self._classify_article(full_text)
        keywords = self.extract_keywords(raw_text or summary, title)

        return {
            'title': title, 'link': link, 'published': published, 'source': source,
            'summary': summary, 'keywords': keywords, 'raw_text': raw_text,
            'main_category': classification['main_category'],
            'sub_category': classification['sub_category'],
            'language': language
        }

    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        feed_url, source, lang = feed_config.get("feed_url"), feed_config.get("source"), feed_config.get("lang")
        logger.info(f"ðŸ“¡ {source}ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìž‘...")
        try:
            response = self.session.get(feed_url, timeout=20)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            if not feed or not feed.entries:
                logger.warning(f"âŒ {source}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return []
            
            articles = [self.process_entry(entry, source, lang) for entry in feed.entries[:20]]
            valid_articles = [article for article in articles if article]
            logger.info(f"âœ… {source}: {len(valid_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ.")
            return valid_articles
        except Exception as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"); return []

    def save_articles(self, articles: List[Dict]) -> Dict:
        stats = {'inserted': 0, 'updated': 0, 'skipped': 0}
        for article in articles:
            try:
                result = db.insert_or_update_article(article); stats[result] += 1
            except Exception as e:
                logger.error(f"DB ì €ìž¥ ì˜¤ë¥˜ ({article.get('link')}): {e}"); stats['skipped'] += 1
        return stats

    def collect_all_news(self, max_feeds: Optional[int] = None) -> Dict:
        logger.info("ðŸš€ ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ìž‘ì—…ì„ ì‹œìž‘í•©ë‹ˆë‹¤.")
        start_time = time.time()
        feeds_to_process = FEEDS[:max_feeds] if max_feeds else FEEDS
        
        all_articles = []
        
        # Process feeds in parallel
        if PARALLEL_MAX_WORKERS > 1:
            with ThreadPoolExecutor(max_workers=min(PARALLEL_MAX_WORKERS, len(feeds_to_process))) as executor:
                future_to_feed = {
                    executor.submit(self.collect_from_feed, feed): feed 
                    for feed in feeds_to_process
                }
                
                for future in as_completed(future_to_feed):
                    try:
                        articles = future.result(timeout=60)  # 60 second timeout per feed
                        all_articles.extend(articles)
                    except Exception as e:
                        feed = future_to_feed[future]
                        logger.error(f"Feed collection failed: {feed.get('source', 'Unknown')}: {e}")
        else:
            # Sequential processing
            for feed in feeds_to_process:
                articles = self.collect_from_feed(feed)
                all_articles.extend(articles)
                
        unique_articles = list({article['link']: article for article in all_articles}.values())
        if unique_articles:
            save_stats = self.save_articles(unique_articles)
        else:
            save_stats = {}
        
        duration = time.time() - start_time
        return {'status': 'success', 'duration': duration, 'stats': save_stats}

collector = EnhancedNewsCollector()
