# backend/enhanced_news_collector.py (ëª¨ë“  ê¸°ëŠ¥ì´ ë³µì›ë˜ê³  ì•ˆì •í™”ëœ ìµœì¢… ë²„ì „)

import logging
import time
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from datetime import datetime
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser
import requests
from bs4 import BeautifulSoup
import dateutil.parser

from database import db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. ì›ë˜ í”„ë¡œì íŠ¸ì˜ ì „ì²´ RSS í”¼ë“œ ëª©ë¡ ë³µì› ---
FEEDS = [
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìì‹ ë¬¸_ì†ë³´", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter", "category": "IT", "lang": "ko"},
    {"feed_url": "https://byline.network/feed/", "source": "Byline Network", "category": "IT", "lang": "ko"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum", "category": "Startup", "lang": "ko"},
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss", "source": "WIRED", "category": "Tech", "lang": "en"},
]

# --- 2. ì›ë˜ í”„ë¡œì íŠ¸ì˜ ê³ ê¸‰ í‚¤ì›Œë“œ ëª©ë¡ ë³µì› ---
STOP_WORDS = {"ê¸°ì", "ë‰´ìŠ¤", "íŠ¹íŒŒì›", "ì˜¤ëŠ˜", "ì‚¬ì§„", "ì˜ìƒ", "ì œê³µ", "ì…ë ¥", "ê²ƒ", "ìˆ˜", "ë“±", "ë°"}
TECH_KEYWORDS = {
    "ai", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "chatgpt", "gpt", "llm", "ìƒì„±í˜•ai",
    "ë°˜ë„ì²´", "semiconductor", "dram", "nand", "hbm", "gpu", "cpu", "npu",
    "ì‚¼ì„±ì „ì", "samsung", "skí•˜ì´ë‹‰ìŠ¤", "tsmc", "ì—”ë¹„ë””ì•„", "nvidia",
    "5g", "6g", "í´ë¼ìš°ë“œ", "cloud", "ë°ì´í„°ì„¼í„°", "ì„œë²„", "server",
    "ë¸”ë¡ì²´ì¸", "blockchain", "ì•”í˜¸í™”í", "metaverse", "ë©”íƒ€ë²„ìŠ¤",
    "ììœ¨ì£¼í–‰", "ì „ê¸°ì°¨", "ev", "ë°°í„°ë¦¬", "battery",
    "ë³´ì•ˆ", "security", "í•´í‚¹", "hacking", "cyber", "ëœì„¬ì›¨ì–´",
    "ì˜¤í”ˆì†ŒìŠ¤", "open source", "ê°œë°œì", "developer", "python", "react",
}

class EnhancedNewsCollector:
    def __init__(self):
        self.session = requests.Session()
        self.stats = {}

    def _extract_keywords(self, text: str, title: str, top_k: int = 15) -> List[str]:
        """[ë³µì›ëœ ê¸°ëŠ¥] ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ê³ ê¸‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not text and not title: return []
        combined_text = f"{title} {text}".lower()
        keywords = {kw for kw in TECH_KEYWORDS if kw in combined_text}
        acronyms = re.findall(r'\b[A-Z]{3,}\b', f"{title} {text}")
        keywords.update(acronyms)
        
        unique_keywords = [kw for kw in list(keywords) if kw.lower() not in STOP_WORDS and len(kw) > 1]
        return unique_keywords[:top_k]

    def _extract_main_text(self, url: str) -> Optional[str]:
        """[ë³µì›ëœ ê¸°ëŠ¥] ê¸°ì‚¬ URLì— ì§ì ‘ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = self.session.get(url, timeout=20, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            for element in soup(["script", "style", "nav", "footer", "aside"]): element.decompose()
            
            content_selectors = ["article", "[class*='article']", "[id*='content']", "main"]
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    main_content = element.get_text(separator="\n", strip=True)
                    if len(main_content) > 200: return main_content
            return None
        except Exception as e:
            logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
            return None

    def _process_entry(self, entry: Dict, source: str, category: str, lang: str) -> Optional[Dict]:
        title = entry.get("title", "No Title").strip()
        link = entry.get("link", "").strip()
        if not title or not link: return None

        try: published = dateutil.parser.parse(entry.get("published", "")).isoformat()
        except: published = datetime.now().isoformat()

        summary = BeautifulSoup(entry.get("summary", ""), "html.parser").get_text(separator=' ', strip=True)
        main_text = self._extract_main_text(link)
        keywords = self._extract_keywords(main_text or summary, title)

        return {'title': title, 'link': link, 'published': published, 'source': source,
                'summary': summary, 'keywords': keywords, 'raw_text': main_text,
                'category': category, 'language': lang}

    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        feed_url, source = feed_config.get("feed_url"), feed_config.get("source", "Unknown")
        logger.info(f"ğŸ“¡ {source}ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        try:
            response = self.session.get(feed_url, timeout=20)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            if not feed or not feed.entries:
                logger.warning(f"âŒ {source}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return []
            
            articles = [self._process_entry(entry, source, feed_config.get("category"), feed_config.get("lang")) for entry in feed.entries[:20]]
            valid_articles = [article for article in articles if article]
            logger.info(f"âœ… {source}: {len(valid_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ.")
            return valid_articles
        except Exception as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"); return []

    def save_articles(self, articles: List[Dict]) -> Dict:
        stats = {'inserted': 0, 'updated': 0, 'skipped': 0}
        for article in articles:
            try:
                result = db.insert_or_update_article(article); stats[result] += 1
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì˜¤ë¥˜ ({article.get('link')}): {e}"); stats['skipped'] += 1
        return stats

    def collect_all_news(self, max_feeds: Optional[int] = None) -> Dict:
        logger.info("ğŸš€ ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        start_time = time.time()
        feeds_to_process = FEEDS[:max_feeds] if max_feeds else FEEDS
        
        all_articles = []
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_feed = {executor.submit(self.collect_from_feed, feed): feed for feed in feeds_to_process}
            for future in as_completed(future_to_feed):
                all_articles.extend(future.result())
        
        unique_articles = list({article['link']: article for article in all_articles}.values())
        if unique_articles:
            save_stats = self.save_articles(unique_articles)
        else:
            save_stats = {}
        
        duration = time.time() - start_time
        return {'status': 'success', 'duration': duration, 'stats': save_stats}

collector = EnhancedNewsCollector()

