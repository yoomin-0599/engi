# backend/enhanced_news_collector.py (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ìµœì¢… ë²„ì „)

import logging
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional
from datetime import datetime

import feedparser
import requests
from bs4 import BeautifulSoup
import dateutil.parser

from database import db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œë¥¼ ì°¸ê³ í•œ ì¹´í…Œê³ ë¦¬ ì‚¬ì „ ì •ì˜ ---
CATEGORIES = {
    "ì²¨ë‹¨ ì œì¡°Â·ê¸°ìˆ  ì‚°ì—…": {
        "ë°˜ë„ì²´": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "dram", "nand", "hbm", "íŒŒìš´ë“œë¦¬", "foundry", "euv"],
        "ìë™ì°¨": ["ìë™ì°¨", "ì „ê¸°ì°¨", "ev", "ìˆ˜ì†Œì°¨", "í•˜ì´ë¸Œë¦¬ë“œ", "ììœ¨ì£¼í–‰", "adas", "ëª¨ë¹Œë¦¬í‹°"],
        "ì´ì°¨ì „ì§€": ["ì´ì°¨ì „ì§€", "2ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ess", "ì „ê³ ì²´", "ncm", "lfp", "ì–‘ê·¹ì¬", "ìŒê·¹ì¬"],
        "ë””ìŠ¤í”Œë ˆì´": ["ë””ìŠ¤í”Œë ˆì´", "oled", "amoled", "lcd", "qd", "ë§ˆì´í¬ë¡œ led"],
        "ë¡œë´‡Â·ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬": ["ë¡œë´‡", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ì‚°ì—…ìš© ë¡œë´‡", "í˜‘ë™ë¡œë´‡", "cobot", "ë””ì§€í„¸íŠ¸ìœˆ"],
    },
    "ë””ì§€í„¸Â·ICT ì‚°ì—…": {
        "AI": ["ai", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìƒì„±í˜• ai", "ì±—ë´‡", "llm"],
        "ICTÂ·í†µì‹ ": ["5g", "6g", "ë„¤íŠ¸ì›Œí¬", "í†µì‹ ", "ìœ„ì„±í†µì‹ ", "í´ë¼ìš°ë“œ", "ë°ì´í„°ì„¼í„°", "ì—£ì§€ ì»´í“¨íŒ…"],
        "ì†Œí”„íŠ¸ì›¨ì–´Â·í”Œë«í¼": ["ì†Œí”„íŠ¸ì›¨ì–´", "ë©”íƒ€ë²„ìŠ¤", "vr", "ar", "xr", "saas", "í•€í…Œí¬", "í”Œë«í¼", "ott", "ê²Œì„", "ë³´ì•ˆ", "ë¹…ë°ì´í„°", "ë¸”ë¡ì²´ì¸"],
    },
}

FEEDS = [
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìì‹ ë¬¸_ì†ë³´"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter"},
    {"feed_url": "https://byline.network/feed/", "source": "Byline Network"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum"},
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge"},
]

class EnhancedNewsCollector:
    def __init__(self):
        self.session = requests.Session()
        self.stats = {}

    def _classify_article(self, title: str, content: str) -> Dict:
        """[ì¶”ê°€ëœ ê¸°ëŠ¥] ê¸°ì‚¬ ì œëª©/ë³¸ë¬¸ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        text = f"{title} {content}".lower()
        best_match = {'score': 0, 'main': 'ê¸°íƒ€', 'sub': 'ê¸°íƒ€'}

        for main_cat, subcats in CATEGORIES.items():
            for sub_cat, keywords in subcats.items():
                score = sum(1 for kw in keywords if kw in text)
                if score > best_match['score']:
                    best_match = {'score': score, 'main': main_cat, 'sub': sub_cat}
        
        return {'main_category': best_match['main'], 'sub_category': best_match['sub']}

    def _extract_keywords_simple(self, text: str, top_n: int = 10) -> List[str]:
        if not text: return []
        text = re.sub(r'[^\w\s]', '', text)
        words = text.split()
        candidates = [word for word in words if len(word) > 1 and not word.isnumeric()]
        stop_words = {"ê¸°ì", "ë‰´ìŠ¤", "ì‚¬ì§„", "ì œê³µ", "ì´ë²ˆ", "ì§€ë‚œ"}
        keywords = [word for word in candidates if word not in stop_words]
        return list(dict.fromkeys(keywords))[:top_n]

    def _process_entry(self, entry: Dict, source: str) -> Optional[Dict]:
        title = entry.get("title", "No Title").strip()
        link = entry.get("link", "").strip()
        if not title or not link: return None

        try: published = dateutil.parser.parse(entry.get("published", "")).isoformat()
        except: published = datetime.now().isoformat()

        summary = BeautifulSoup(entry.get("summary", ""), "html.parser").get_text(separator=' ', strip=True)
        keywords = self._extract_keywords_simple(f"{title} {summary}")
        
        # [ì¶”ê°€] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤í–‰
        classification = self._classify_article(title, summary)

        article_data = {
            'title': title, 'link': link, 'published': published,
            'source': source, 'summary': summary, 'keywords': keywords,
            'main_category': classification['main_category'],
            'sub_category': classification['sub_category'],
        }
        return article_data

    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        feed_url, source = feed_config.get("feed_url"), feed_config.get("source", "Unknown")
        logger.info(f"ğŸ“¡ {source}ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        try:
            response = self.session.get(feed_url, timeout=20)
            response.raise_for_status()
            feed = feedparser.parse(response.content)
            if not feed or not feed.entries:
                logger.warning(f"âŒ {source}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return []
            
            articles = [self._process_entry(entry, source) for entry in feed.entries[:20]]
            valid_articles = [article for article in articles if article]
            logger.info(f"âœ… {source}: {len(valid_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ.")
            return valid_articles
        except Exception as e:
            logger.error(f"âŒ {source} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"); return []

    def save_articles(self, articles: List[Dict]) -> Dict:
        stats = {'inserted': 0, 'updated': 0, 'skipped': 0}
        for article in articles:
            try:
                result = db.insert_or_update_article(article); stats[result] += 1
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì˜¤ë¥˜ ({article.get('link')}): {e}"); stats['skipped'] += 1
        return stats

    def collect_all_news(self, max_feeds: Optional[int] = None) -> Dict:
        logger.info("ğŸš€ ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        start_time = time.time()
        feeds_to_process = FEEDS[:max_feeds] if max_feeds else FEEDS
        
        all_articles = []
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_feed = {executor.submit(self.collect_from_feed, feed): feed for feed in feeds_to_process}
            for future in as_completed(future_to_feed):
                all_articles.extend(future.result())
        
        unique_articles = list({article['link']: article for article in all_articles}.values())
        if unique_articles:
            save_stats = self.save_articles(unique_articles)
        else:
            save_stats = {}
        
        duration = time.time() - start_time
        return {'status': 'success', 'duration': duration, 'stats': save_stats}

collector = EnhancedNewsCollector()

